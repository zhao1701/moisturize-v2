{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:45:06.470638Z",
     "start_time": "2019-06-19T21:45:06.452720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '15'\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T23:41:34.051068Z",
     "start_time": "2019-06-19T23:41:34.015862Z"
    }
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "def _calc_gaussian_log_density(data, mu, log_sigma):\n",
    "    \"\"\"\n",
    "    Given data samples and their distributions, calculate each samples' log\n",
    "    density. Gaussian probability density function:\n",
    "\n",
    "      p(x) = (1 / (sigma * sqrt(2 * pi))) * exp(-0.5 * (x - mu)/sigma)^2)\n",
    "\n",
    "    Then:\n",
    "\n",
    "      log[p(x)] = -log[sigma] - 0.5 * log[2 * pi] - 0.5 * [(x - mu)/sigma]^2\n",
    "      log[p(x)] = -0.5 * [(x - mu / sigma)^2 + 2 * log[sigma] + log[2 * pi]]\n",
    "\n",
    "    Let:\n",
    "\n",
    "      c = log[2 * pi]\n",
    "      inv_sigma = 1 / sigma\n",
    "      tmp = (x - mu) / sigma = (x - mu) * inv_sigma\n",
    "\n",
    "    Then:\n",
    "\n",
    "      log[p(x)] = -0.5 * [tmp * tmp + 2 * log[sigma] + c]\n",
    "    \"\"\"\n",
    "    \n",
    "    c = np.log(2 * np.pi)\n",
    "    inv_sigma = K.exp(-log_sigma)\n",
    "    tmp = (data - mu) * inv_sigma\n",
    "    log_density = -0.5 * (tmp*tmp + 2*log_sigma + c)\n",
    "    return log_density\n",
    "\n",
    "\n",
    "def _logsumexp(value, axis, keepdims=False):\n",
    "    \"\"\"\n",
    "    A numerically stable computation for chaining the operations: log, sum, and\n",
    "    exp.\n",
    "    \n",
    "        log[sum_i(exp(x_i))]\n",
    "        = m - m + log[sum_i(exp(x_i))]\n",
    "        = m + log[1/exp(m)] + log[sum_i(exp(x_i))]\n",
    "        = m + log[(1/exp(m))*sum_i(exp(x_i))]\n",
    "        = m + log[sum_i(exp(x_i)/exp(m))]\n",
    "        = m + log[sum_i(exp(x_i-m))]\n",
    "    \"\"\"\n",
    "    \n",
    "    m = K.max(value, axis=axis, keepdims=True)\n",
    "    _value = value - m\n",
    "    if not keepdims:\n",
    "        m = K.squeeze(m, axis=axis)\n",
    "    result = m + K.log(K.sum(K.exp(_value), axis=axis, keepdims=keepdims))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _log_importance_weight_matrix(batch_size, dataset_size):\n",
    "    \"\"\"\n",
    "    Returns a weight matrix that is used to estimate the unconditional latent\n",
    "    distribution q(z).\n",
    "    \"\"\"\n",
    "    N = dataset_size\n",
    "    M = batch_size - 1\n",
    "    strat_weight = (N - M) / (N * M)\n",
    "    W = K.ones(shape=(batch_size, batch_size))\n",
    "    W.fill(1 / M)\n",
    "    W = W * 1/m\n",
    "    W[:, 0] = 1 / N\n",
    "    W[:, 1] = strat_weight\n",
    "    W[M-1, 0] = strat_weight\n",
    "    W = np.log(W)\n",
    "    return W\n",
    "\n",
    "\n",
    "def mutual_information_index(batch_size, dataset_size):\n",
    "    \n",
    "    def mutual_information_index(z, z_mu, z_log_sigma, **_):\n",
    "        \n",
    "        # Calculate log densities for sampled latent vectors given the\n",
    "        # distributions. Generated by the encoder network from the input\n",
    "        # images (aka q(z|x)).\n",
    "        logqz_condx = _calc_gaussian_log_density(\n",
    "            z, z_mu, z_log_sigma)\n",
    "        logqz_condx = K.sum(logqz_condx, axis=1)\n",
    "        \n",
    "        # Calculate the log densities from the aggregate latent posterior\n",
    "        # distribution q(z).\n",
    "        # log q(z) ~= log 1 /(NM) sum_m=1^M q(z|x_m)\n",
    "        # = - log(MN) + logsumexp_m(q(z|x_m))\n",
    "        _logqz = _calc_gaussian_log_density(\n",
    "            K.reshape(z, shape=(-1, 1, int(z.shape[1]))),\n",
    "            K.reshape(z_mu, shape=(-1, 1, int(z_mu.shape[1]))),\n",
    "            K.reshape(z_log_sigma, shape=(-1, 1, int(z_log_sigma.shape[1]))))\n",
    "        \n",
    "        # Estimate log[q(z)]\n",
    "        logiw_matrix = _log_importance_weight_matrix(batch_size, dataset_size)\n",
    "        logqz = _logsumexp(\n",
    "            logiw_matrix + K.sum(_logqz, axis=2), axis=1, keepdims=False)\n",
    "        \n",
    "        # This tensor corresponds to but is not equivalent to the mi_index term\n",
    "        # of the decomposed divergence penalty. However, minimizing this\n",
    "        # serves to minimize the true penalty term.\n",
    "        mi_index = K.mean(logqz_condx - logqz)\n",
    "        return mi_index\n",
    "    \n",
    "    return mutual_information_index\n",
    "\n",
    "\n",
    "def total_correlation(batch_size, dataset_size):\n",
    "    \n",
    "    def total_correlation(z, z_mu, z_log_sigma, **_):\n",
    "        \n",
    "        # Calculate the log densities from the aggregate latent posterior\n",
    "        # distribution q(z).\n",
    "        # log q(z) ~= log 1 /(NM) sum_m=1^M q(z|x_m)\n",
    "        # = - log(MN) + logsumexp_m(q(z|x_m))\n",
    "        _logqz = _calc_gaussian_log_density(\n",
    "            K.reshape(z, shape=(-1, 1, int(z.shape[1]))),\n",
    "            K.reshape(z_mu, shape=(-1, 1, int(z_mu.shape[1]))),\n",
    "            K.reshape(z_log_sigma, shape=(-1, 1, int(z_log_sigma.shape[1]))))\n",
    "        \n",
    "        # Estimate log[q(z)]\n",
    "        logiw_matrix = _log_importance_weight_matrix(batch_size, dataset_size)\n",
    "        logqz = _logsumexp(\n",
    "            logiw_matrix + K.sum(_logqz, axis=2), axis=1, keepdims=False)\n",
    "        \n",
    "        # Estimate log[prod_j[q(z_j)]]\n",
    "        shape = (batch_size, batch_size, 1)\n",
    "        logqz_prod_marginals = _logsumexp(\n",
    "            K.reshape(logiw_matrix, shape=shape) + _logqz,\n",
    "            axis=1, keepdims=False)\n",
    "        logqz_prod_marginals = K.sum(logqz_prod_marginals, axis=1)\n",
    "        \n",
    "        # This tensor corresponds to but is not equivalent to the total\n",
    "        # correlation term of the decomposed divergence penalty. However,\n",
    "        # minimizing this serves to minimize the true penalty term.\n",
    "        tc = K.mean(logqz - logqz_prod_marginals)\n",
    "        return tc\n",
    "    \n",
    "    return total_correlation\n",
    "\n",
    "\n",
    "def dimensional_kl(batch_size, dataset_size):\n",
    "    \n",
    "    def dimensional_kl(z, z_mu, z_log_sigma, **_):\n",
    "        \n",
    "        # Calculate log densities for sampled latent vectors in a standard\n",
    "        # Gaussian distribution (zero mean, unit variance)\n",
    "        logpz = _calc_gaussian_log_density(\n",
    "          z, K.zeros_like(z_mu), K.zeros_like(z_log_sigma))\n",
    "        logpz = K.sum(logpz, axis=1)\n",
    "        \n",
    "        # Estimate log[prod_j[q(z_j)]]\n",
    "        _logqz = _calc_gaussian_log_density(\n",
    "            K.reshape(z, shape=(-1, 1, int(z.shape[1]))),\n",
    "            K.reshape(z_mu, shape=(-1, 1, int(z_mu.shape[1]))),\n",
    "            K.reshape(z_log_sigma, shape=(-1, 1, int(z_log_sigma.shape[1]))))\n",
    "        logiw_matrix = _log_importance_weight_matrix(batch_size, dataset_size)\n",
    "        shape = (batch_size, batch_size, 1)\n",
    "        logqz_prod_marginals = _logsumexp(\n",
    "            K.reshape(logiw_matrix, shape=shape) + _logqz,\n",
    "            axis=1, keepdims=False)\n",
    "        logqz_prod_marginals = K.sum(logqz_prod_marginals, axis=1)\n",
    "        \n",
    "        # This tensor corresponds to but is not equivalent to the dimensional\n",
    "        # KL term of the decomposed divergence penalty. However,\n",
    "        # minimizing this serves to minimize the true penalty term.     \n",
    "        dim_kl = K.mean(logqz_prod_marginals - logpz)\n",
    "        return dim_kl\n",
    "    \n",
    "    return dimensional_kl   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T21:42:26.421400Z",
     "start_time": "2019-06-19T21:42:26.403447Z"
    }
   },
   "outputs": [],
   "source": [
    "def _check_loss_fn(loss_fn, batch_size, dataset_size):\n",
    "    \"\"\"\n",
    "    Converts TCVAE latent loss functions into a format similar to that\n",
    "    of reconstruction and VAE latent loss functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    fn_signature = tuple(signature(loss_fn).parameters.keys())\n",
    "    if 'batch_size' in fn_signature or 'dataset_size' in fn_signature:\n",
    "        return loss_fn(batch_size, dataset_size)\n",
    "    else:\n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T23:39:23.763109Z",
     "start_time": "2019-06-19T23:39:23.740183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_5:0' shape=(32, 100) dtype=float64>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.reshape(np.random.randn(100, 32), shape=(32, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T00:05:57.818289Z",
     "start_time": "2019-06-20T00:05:57.798289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_3:0' shape=(32, 32, 1) dtype=float32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.cast(np.expand_dims(np.random.randn(32, 32), axis=-1), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
